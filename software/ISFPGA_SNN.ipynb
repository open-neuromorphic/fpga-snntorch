{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/open-neuromorphic/fpga-snntorch/blob/main/software/ISFPGA_SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w6lhn7H8fW5"
      },
      "source": [
        "# ISFPGA Workshop\n",
        "## Who needs neuromorphic hardware? Deploying SNNs to FPGAs via HLS Open-Source Neuromorphic Circuit Design\n",
        "### By Jason K. Eshraghian (www.ncg.ucsc.edu)\n",
        "\n",
        "\n",
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"200\">](https://github.com/jeshraghian/snntorch/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BlTqunB73-t"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch --quiet # shift + enter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLlnINAI9mgJ"
      },
      "source": [
        "*What will I learn?*\n",
        "\n",
        "1. Train an SNN classifier using snnTorch\n",
        "2. Hardware Friendly Training\n",
        "  - Weight Quantization with Brevitas\n",
        "  - Stateful Quantization\n",
        "3. Handling neuromorphic data with Tonic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CS50cwuCW6n"
      },
      "source": [
        "# 1. Train an SNN Classifier using snnTorch\n",
        "## 1.1 Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_TzogsCCcSe"
      },
      "outputs": [],
      "source": [
        "# snntorch imports\n",
        "import snntorch as snn\n",
        "from snntorch import functional as SF\n",
        "\n",
        "# pytorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# data manipulation\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nftOdpyAGv7D"
      },
      "source": [
        "## 1.2 Boilerplate: DataLoading the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsM2Z5NXGu5z"
      },
      "outputs": [],
      "source": [
        "# dataloader arguments\n",
        "batch_size = 128\n",
        "data_path='/data/mnist'\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "## if you're on M1 or M2 GPU:\n",
        "# device = torch.device(\"mps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqbYptgDHUPg"
      },
      "outputs": [],
      "source": [
        "# Define a transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSlS3gWZHXI0"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJVhfNukHbsp"
      },
      "source": [
        "## 1.3 Construct SNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu324fr_HhxV"
      },
      "outputs": [],
      "source": [
        "# Network Architecture\n",
        "num_inputs =\n",
        "num_hidden =\n",
        "num_outputs =\n",
        "\n",
        "# Temporal Dynamics\n",
        "num_steps =\n",
        "beta ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkM1Z1EjHeW8"
      },
      "outputs": [],
      "source": [
        "from snntorch import surrogate\n",
        "\n",
        "# Define Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 =\n",
        "        self.lif1 =\n",
        "        self.fc2 =\n",
        "        self.lif2 =\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        # time-loop\n",
        "        for step in range(num_steps):\n",
        "          cur1 = self.fc1(...) # batch: 128 x 784\n",
        "          spk1, mem1 = self.lif1(...)\n",
        "          cur2 = self.fc2(...)\n",
        "          spk2, mem2 = self.lif2(...)\n",
        "\n",
        "          # store in list\n",
        "          spk2_rec.append(spk2)\n",
        "          mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0) # time-steps x batch x num_out\n",
        "\n",
        "# Load the network onto CUDA if available\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8qBw03rHpn3"
      },
      "source": [
        "## 1.4 Training the SNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1telBMU-HrIg"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, dataloader, num_epochs=1):\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
        "  counter = 0\n",
        "\n",
        "  # Outer training loop\n",
        "  for epoch in range(num_epochs):\n",
        "      train_batch = iter(dataloader)\n",
        "\n",
        "      # Minibatch training loop\n",
        "      for data, targets in train_batch:\n",
        "          data = data.to(device)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          # forward pass\n",
        "          model.train()\n",
        "          spk_rec, _ = model(data)\n",
        "\n",
        "          # initialize the loss & sum over time\n",
        "          loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "          loss_val = loss(spk_rec.sum(0), targets) # batch x num_out\n",
        "\n",
        "          # Gradient calculation + weight update\n",
        "          optimizer.zero_grad()\n",
        "          loss_val.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Print train/test loss/accuracy\n",
        "          if counter % 10 == 0:\n",
        "              print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
        "          counter += 1\n",
        "\n",
        "          if counter == 100:\n",
        "            break\n",
        "\n",
        "training_loop(net, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHsdkuSlIS1E"
      },
      "outputs": [],
      "source": [
        "def measure_accuracy(model, dataloader):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    running_length = 0\n",
        "    running_accuracy = 0\n",
        "\n",
        "    for data, targets in iter(dataloader):\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # forward-pass\n",
        "      spk_rec, _ = model(data)\n",
        "      spike_count = spk_rec.sum(0) # batch x num_outputs\n",
        "      _, max_spike = spike_count.max(1)\n",
        "\n",
        "      # correct classes for one batch\n",
        "      num_correct = (max_spike == targets).sum()\n",
        "\n",
        "      # total accuracy\n",
        "      running_length += len(targets)\n",
        "      running_accuracy += num_correct\n",
        "\n",
        "    accuracy = (running_accuracy / running_length)\n",
        "\n",
        "    return accuracy.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJHAltRCKGyx"
      },
      "outputs": [],
      "source": [
        "print(f\"Test set accuracy: {measure_accuracy(net, test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5Nc7TTxZWTp"
      },
      "source": [
        "### A Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct-kV1rZZVb3"
      },
      "outputs": [],
      "source": [
        "def print_sample(model, dataloader, idx=0):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "\n",
        "    data, targets = next(iter(dataloader))\n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # forward-pass\n",
        "    spk_rec, _ = model(data)\n",
        "    spike_count = spk_rec.sum(0) # batch x num_outputs\n",
        "    _, max_spike = spike_count.max(1)\n",
        "\n",
        "    # Plot the sample\n",
        "    plt.imshow(data[idx].cpu().squeeze(), cmap='gray')\n",
        "    plt.title(f'Target: {targets[idx].item()}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPtBj7gtadA2"
      },
      "outputs": [],
      "source": [
        "print_sample(net, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9vrb2zUD6S-"
      },
      "source": [
        "# 2. Hardware Friendly Training\n",
        "## 2.1 Weight Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icK4WzuL-2QA"
      },
      "outputs": [],
      "source": [
        "!pip install brevitas --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idiLnVjJGJAL"
      },
      "source": [
        "Just replace all `nn.Linear` layers with `qnn.QuantLinear(num_inputs, num_outputs, weight_bit_width, bias)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiZYTMA6D-YL"
      },
      "outputs": [],
      "source": [
        "import brevitas.nn as qnn\n",
        "\n",
        "# Define Network\n",
        "class QuantNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 =\n",
        "        self.lif1 =\n",
        "        self.fc2 =\n",
        "        self.lif2 =\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x.flatten(1))\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "# Load the network onto CUDA if available\n",
        "qnet = QuantNet().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PocSa27MOOoK"
      },
      "outputs": [],
      "source": [
        "training_loop(qnet, train_loader)\n",
        "print(f\"Test set accuracy: {measure_accuracy(qnet, test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvRzLHdjD-6S"
      },
      "source": [
        "## 2.2 SQUAT: Stateful Quantization-Aware Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAaafUczEAu_"
      },
      "outputs": [],
      "source": [
        "from snntorch.functional import quant\n",
        "\n",
        "# Define Network\n",
        "class SquatNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define state quantization parameters\n",
        "        q_lif = quant.state_quant(num_bits=4, uniform=True)\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 =\n",
        "        self.lif1 =\n",
        "        self.fc2 =\n",
        "        self.lif2 =\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x.flatten(1))\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "# Load the network onto CUDA if available\n",
        "sqnet = SquatNet().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zfo0h6AQDzS"
      },
      "outputs": [],
      "source": [
        "training_loop(sqnet, train_loader)\n",
        "print(f\"Test set accuracy: {measure_accuracy(sqnet, test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZAFVgHW4M2g"
      },
      "source": [
        "# 3. Handling Neuromorphic Data with Tonic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJvN9EH7b7XL"
      },
      "outputs": [],
      "source": [
        "!pip install tonic --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB9oK66mcCYt"
      },
      "source": [
        "## 3.1 PokerDVS Dataset\n",
        "\n",
        "The dataset used in this tutorial is POKERDVS by T. Serrano-Gotarredona and B. Linares-Barranco:\n",
        "\n",
        "```\n",
        "Serrano-Gotarredona, Teresa, and Bernab√© Linares-Barranco. \"Poker-DVS and MNIST-DVS. Their history, how they were made, and other details.\" Frontiers in neuroscience 9 (2015): 481.\n",
        "```\n",
        "\n",
        "It is comprised of four classes, each being a suite of a playing card deck: clubs, spades, hearts, and diamonds. The data consists of 131 poker pip symbols, and was collected by flipping poker cards in front of a DVS128 camera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-TSV9h2hGkW"
      },
      "outputs": [],
      "source": [
        "import tonic\n",
        "\n",
        "poker_train = tonic.datasets.POKERDVS(save_to='./data', train=True)\n",
        "poker_test = tonic.datasets.POKERDVS(save_to='./data', train=False)\n",
        "\n",
        "events, target = poker_train[0]\n",
        "print(events)\n",
        "tonic.utils.plot_event_grid(events)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tcz7CE4wb6Gs"
      },
      "outputs": [],
      "source": [
        "import tonic.transforms as transforms\n",
        "from tonic import DiskCachedDataset\n",
        "\n",
        "# time_window\n",
        "frame_transform = tonic.transforms.Compose([tonic.transforms.Denoise(filter_time=10000),\n",
        "                                            tonic.transforms.ToFrame(\n",
        "                                            sensor_size=tonic.datasets.POKERDVS.sensor_size,\n",
        "                                            time_window=1000)\n",
        "                                            ])\n",
        "\n",
        "batch_size = 8\n",
        "cached_trainset = DiskCachedDataset(poker_train, transform=frame_transform, cache_path='./cache/pokerdvs/train')\n",
        "cached_testset = DiskCachedDataset(poker_test, transform=frame_transform, cache_path='./cache/pokerdvs/test')\n",
        "\n",
        "train_loader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "test_loader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "\n",
        "data, labels = next(iter(train_loader))\n",
        "print(data.size())\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6h_YU0zhrs3"
      },
      "source": [
        "## 3.2 Construct Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HEhzeA02Ueg"
      },
      "outputs": [],
      "source": [
        "import torch.functional as F\n",
        "\n",
        "# Define Network\n",
        "class DVSNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        beta = 0.9\n",
        "\n",
        "        # Initialize layers\n",
        "        self.conv1  =\n",
        "        self.mp1    =\n",
        "        self.lif1   =\n",
        "        self.conv2  =\n",
        "        self.mp2    =\n",
        "        self.lif2   =\n",
        "        self.fc     =\n",
        "        self.lif3   =\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        for step in range(...):\n",
        "            cur1       =\n",
        "            spk1, mem1 =\n",
        "            cur2       =\n",
        "            spk2, mem2 =\n",
        "            cur3       =\n",
        "            spk3, mem3 =\n",
        "\n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "# Load the network onto CUDA if available\n",
        "dvsnet = DVSNet().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5havFWd3c-F"
      },
      "outputs": [],
      "source": [
        "training_loop(dvsnet, train_loader, num_epochs=10)\n",
        "print(f\"Test set accuracy: {measure_accuracy(dvsnet, test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNrFs3ro-xUm"
      },
      "source": [
        "That's all folks!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
